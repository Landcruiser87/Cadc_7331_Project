{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# The Wonderful World of Coffee\n",
    "### Business Understanding\n",
    "This notebook begins to explore a coffee dataset that has been scraped by a reddit user from the Coffee Quality\n",
    " Institute's website and uploaded onto GitHub.  Our idea behind researching this dataset is to identify where the top\n",
    "coffee brands come from, and what attributes go into the production of that coffee that makes it so desirable?\n",
    "Can a model be built targeting those ranges of successful coffee producers in order to predict ratings for \n",
    "their future brands?  These are some of the questions we will investigate in our first project.\n",
    "\n",
    "The data source for our dataset:\n",
    "\n",
    "https://github.com/jldbc/coffee-quality-database\n",
    "\n",
    "### Data Description (Meaning/Type/Quality)\n",
    "Lets import our libraries and data. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Add library references\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Upload Data\n",
    "df_ar = pd.read_csv('https://raw.githubusercontent.com/jldbc/coffee-quality-database/master/data/arabica_data_cleaned.csv',\n",
    "                    sep=',', header=0) # read in the arabicaica data\n",
    "df_rob = pd.read_csv('https://raw.githubusercontent.com/jldbc/coffee-quality-database/master/data/robusta_data_cleaned.csv',\n",
    "                     sep=',', header=0) # read in the Robusta data\n",
    "#Column rename to match for merging\n",
    "df_ar.rename(columns={'Unnamed: 0':'Id'}, inplace=True)\n",
    "df_rob.rename(columns={'Unnamed: 0':'Id',\n",
    "                       'Bitter...Sweet':'Sweetness',\n",
    "                       'Uniform.Cup':'Uniformity',\n",
    "                       'Salt...Acid':'Acidity',\n",
    "                       'Fragrance...Aroma':'Aroma'}, inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data meaning\n",
    "\n",
    "Below is a list of continuous and categorical measures:\n",
    "#### Continuous (Quality) Measures\n",
    "* Aroma  \n",
    "* Flavor\n",
    "* Aftertaste\n",
    "* Acidity\n",
    "* Body\n",
    "* Balance\n",
    "* Uniformity\n",
    "* Cup Cleanliness\n",
    "* Sweetness\n",
    "* Moisture\n",
    "* Defects\n",
    "* Cupper Points\n",
    "* Total Cup Points\n",
    "\n",
    "#### Categorical (Bean) Measures\n",
    "* Processing Method\n",
    "* Color\n",
    "* Species (arabica / robusta)\n",
    "\n",
    "#### Categorical (Farm) Measures\n",
    "* Owner\n",
    "* Country of Origin\n",
    "* Farm Name\n",
    "* Lot Number\n",
    "* Mill\n",
    "* Company\n",
    "* Altitude\n",
    "* Region\n",
    "\n",
    "Since the data came to us in two CSV's of arabica and robusta, lets combine the two datasets to begin our analysis.  First we'll need to remove \n",
    "a few columns and merge the two dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "#dropping columns we won't use\n",
    "df_rob = df_rob.drop(['Lot.Number', 'altitude_low_meters', 'altitude_high_meters', 'Certification.Contact',\n",
    "                      'Certification.Contact', 'Expiration', 'Certification.Body', 'ICO.Number',\n",
    "                      'Certification.Address','Mouthfeel', 'Id'], axis=1)\n",
    "df_ar = df_ar.drop(['Lot.Number', 'altitude_low_meters', 'altitude_high_meters', 'Certification.Contact',\n",
    "                      'Certification.Contact', 'Expiration', 'Certification.Body', 'ICO.Number',\n",
    "                      'Certification.Address', 'Id'], axis=1)\n",
    "\n",
    "df_comb = df_ar.append(df_rob)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Quality & Simple Statistics\n",
    "Now that our dataframes are combined, lets analyze counts of missing values and simple statistics.\n",
    "#### Missing Values\n",
    "The majority of missing values center around farm name, mill, producer, altitude, company.  At this stage, we need\n",
    "to decide what categorical values we can keep for regional analysis.  Currently its looking like country might be one\n",
    "of the better attributes to start an analysis.  Luckily, the continuous data has very low NA counts which means any \n",
    "regression, will likely fair well.  Yet with the categorical attributes missing a fair amount of data.  Using them \n",
    "for classification analysis might be difficult without sacrificing data integrity.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Structure of data:\\n\",df_comb.shape,\"\\n\")\n",
    "print(\"Count of missing values:\\n\",df_comb.isnull().sum().sort_values(ascending=False),\"\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Statistics\n",
    "As any good Data Scientist, we first must check our data ranges, means, max's, mins, and quartiles, to see where\n",
    "the data sits.  Our continuous variables operate on a scale from 1 to 10, meaning most values\n",
    "won't need to be imputed.  Any, that are, can easily be dropped without effecting the sample size.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Simple Stats\n",
    "# print(df_comb.head().append(df_comb.tail()), \"\\n\")\n",
    "print(\"Summary Statistic's:\\n\",round(df_comb.describe(),2),\"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking through the statistics we find a few outliers in the altitude mean category so we replaced them with an Na and then imputed the \n",
    "rest of the NA's for altitude with the mean of the attribute. We also removed a row that didn't have a country\n",
    "of origin.  Most of the other simple statistics seem to be in their appropriate ranges for what we expect on the continuous\n",
    "variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Changing datatypes\n",
    "conv_dict = {'Species': str,\n",
    "                'Owner': str,\n",
    "                'Mill': str,\n",
    "                'Company': str,\n",
    "                'Region': str,\n",
    "                'Producer': str,\n",
    "                'Variety': str\n",
    "                }\n",
    "df_comb = df_comb.astype(conv_dict)\n",
    "#Outlier Removal altitude\n",
    "df_comb.loc[[896,1040,1144,543],'altitude_mean_meters'] = np.nan\n",
    "df_comb['altitude_mean_meters'].fillna((df_comb['altitude_mean_meters'].mean()),inplace=True)\n",
    "#nan removal from country\n",
    "df_comb = df_comb.drop(df_comb.index[1197])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Attributes\n",
    "Now that we've got our data organized and a little cleaner, lets begin visualizing our data. We'll start with a \n",
    "bar chart of the Number of coffee samples by Country.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "counts = df_comb['Country.of.Origin'].value_counts().to_dict()\n",
    "min_count = min(counts.values())\n",
    "max_count = max(counts.values())\n",
    "\n",
    "#Bar graph of number of coffee samples per country, top 50\n",
    "counts_top50 = dict(list(counts.items())[int(len(counts)/2):])\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.bar(range(len(counts)), list(counts.values()), align='center')\n",
    "plt.xticks(range(len(counts)), list(counts.keys()), rotation=90)\n",
    "plt.title(\"Number of Coffee Samples by Country\", fontsize='18')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With Mexico coming in first, we're not surprised by those that follow it.  Columbia, Guatamala, Brazil, Taiwan are\n",
    "all excellent climates for growing coffee beans.  This matches with common knowledge of larger coffee producing\n",
    "countries.  Another interesting side-note is that Hawaii produces 3x as many coffee varieties as the mainland US.  \n",
    "\n",
    "\n",
    "Now that we've got an idea behind who are the biggest producers, lets see who produces some of the better brands \n",
    "via the reviewers \"Total Cup Score.\"  We'll group by country and analyze their average score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Dataframe of countries by avg Cup rating\n",
    "\n",
    "country_lists=list(df_comb['Country.of.Origin'].unique())\n",
    "country_total_cup_ratio=[]\n",
    "for each in country_lists:\n",
    "    country=df_comb[df_comb['Country.of.Origin']==each]\n",
    "    country_total_cup_avg=round(sum(country['Total.Cup.Points'])/len(country),2)\n",
    "    country_total_cup_ratio.append(country_total_cup_avg)\n",
    "\n",
    "    \n",
    "data=pd.DataFrame({'Country of Origin':country_lists,'Total Cup Avg':country_total_cup_ratio})\n",
    "new_index=(data['Total Cup Avg'].sort_values(ascending=False)).index.values\n",
    "sorted_data = data.reindex(new_index)\n",
    "sorted_data.head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we see a few differing results as alot of the central american countries are lower than expected in \n",
    "terms of overall coffee rating.  The best coffee it would seem comes from Papua New Guinea, Ethiopia, and Japan, \n",
    "and the United States.  \n",
    "\n",
    "Now that we've got a few countries of interest, lets delve into the continuous variables themselves to get an \n",
    "idea behind their distributions.  So first a frequency plot of the continuous variables we initially predict to \n",
    "be relevant.  We'll also do a pairplot to view some scatterplots of the same variables.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#some initial plots \n",
    "col_names = ['Aroma','Aftertaste', 'Aroma','Balance', \n",
    "             'Flavor', 'Acidity','Moisture', 'Cupper.Points', 'Total.Cup.Points']\n",
    "\n",
    "fig, ax = plt.subplots(len(col_names), figsize=(16,12))\n",
    "\n",
    "for i, col_val in enumerate(col_names):\n",
    "\n",
    "    sns.distplot(df_comb[col_val], hist=True, ax=ax[i])\n",
    "    ax[i].set_title('Freq dist '+col_val, fontsize=10)\n",
    "    ax[i].set_xlabel(col_val, fontsize=8)\n",
    "    ax[i].set_ylabel('Count', fontsize=8)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# #Huge pairplot matrix.  Probably need to whittle down the attributes a bit first.Example drops below\n",
    "sns.pairplot(df_comb, vars = ['Aroma','Aftertaste', 'Aroma','Balance', \n",
    "             'Flavor', 'Acidity','Moisture', 'Cupper.Points', 'Total.Cup.Points'],\n",
    "             hue = 'Species');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've got a better higher level view of the data, we begin to see clustering in the continuous\n",
    "variables around the 6 to 9 rating mark.  Add to that our range for the rating scale is 1 to 10 and we begin to see\n",
    "some problems with our dataset.  Mainly since our variables have a very similar distribution, we could end up with\n",
    "a highly correlated dataset.  So our next step is to look at some individual histograms and\n",
    "a correlation heat map to confirm our suspicions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df_num = df_comb.select_dtypes(include=['float64'])\n",
    "df_num.hist(figsize =(14,12))\n",
    "#Generate Color Map\n",
    "colormap = sns.diverging_palette(220, 10, as_cmap=True)\n",
    "f, ax = plt.subplots(figsize=(10, 10))\n",
    "corr = df_comb.corr()\n",
    "sns.heatmap(corr, cmap=\"coolwarm\", annot=True, fmt=\".1f\",\n",
    "            xticklabels=corr.columns.values,\n",
    "            yticklabels=corr.columns.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is some correlation to be concerned with, but for now lets just keep it in mind.\n",
    "This sucks. i hate bee's\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
